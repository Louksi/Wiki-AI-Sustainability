# Assigment 2

## 1. What machine learning methods can be used?


Several machine learning techniques are applicable to drone-based crop disease detection and treatment. The most common approach is supervised learning, where models are trained on labeled images of healthy and diseased crops to classify new samples. Algorithms such as Random Forests or Support Vector Machines are often used for this purpose. More advanced methods rely on deep learning, particularly convolutional neural networks (CNNs), which can automatically learn relevant visual features from images captured by RGB, multispectral, or hyperspectral cameras. 
This is particularly the case for the identification of diseased areas in images of vines where certain drones employ Deep Learning using two neural networks: LaNet5 to pre-label the images (helps create the training dataset) and SegNet to learn to segment visible and infrared images into 4 classes: shade, soil, healthy leaf, diseased leaf. [^1]
Other architectures like ResNet, EfficientNet, or U-Net are frequently employed for classification, segmentation, or object detection tasks. When labeled data is unavailable, unsupervised learning methods such as clustering or dimensionality reduction (for example, k-Means or PCA) can help detect anomalies or unusual vegetation patterns. In some cases, reinforcement learning is also used to optimize drone navigation and flight paths, improving coverage efficiency and reducing energy consumption.
For precision orchard management, we can use Max-Relevance and Min-Redundancy (mRMR) algorithm “to select three features (RVI, NDVI, SAVI)” that enhance the distinction between tree canopies and background noise. A “feature fusion method [...]” can be “[...] utilized to enhance image details.”. Such a method was used in the YOLOv9 with the FasterNet module to extract features from input data, and added the iRMB attention mechanism module downstream throughout the entire network. This improved YOLOv9 model was named YOLO-Fi. The ant-colony algorithm is a bio-inspired optimization used for improved path-findings. [^2]


## 2. What equipment or infrastructure is needed?

The hardware and infrastructure required to implement these systems combine aerial, computational, and software components. Drones such as quadcopters, hexacopters, or fixed-wing models are equipped with imaging sensors, including RGB, multispectral, hyperspectral, thermal, or LiDAR cameras, as well as GPS modules for precise geolocation. When treatment is automated, spraying mechanisms may also be integrated. Data is collected and managed through a ground control station that plans flights and receives telemetry information. For processing, drones can rely on onboard edge devices, such as NVIDIA Jetson modules, to perform real-time inference. Larger analyses, including model training and post-flight processing, are executed on local servers or cloud platforms such as AWS, Google Cloud, or Microsoft Azure. The software stack typically involves image and data processing tools like OpenCV, GDAL, or QGIS, and machine learning frameworks such as TensorFlow, PyTorch, or scikit-learn.
The UAV system used with the YOLO-Fi system relies on UAV-based data collection setup. Specifically, “The Jy-DGP_X5 drone, equipped with the MS 600 V2 multispectral camera [...]” provides the raw multispectral imagery. Ground truth data are collected via “GCPs [...] using the M6 II RTK mobile station [...] These GCPs were utilized in Pix4D Mapper software [...] to correct geographic positions, generating a DOM with a ground resolution of 1.6 cm” The infrastructure also includes computational resources for AI model training. The original paper lists “GPU NVIDIA GeForce RTX 4090” and the PyTorch framework, ensuring the high computational power needed for training deep learning models [^2].


# 3. What kind of data is used, and how is it managed?

The data used in these applications mainly consists of images acquired by the drones-RGB, multispectral, hyperspectral, or thermal-along with metadata describing the GPS position, altitude, and weather conditions during each capture. The volume of data can be significant, often reaching several tens or hundreds of gigabytes per flight depending on image resolution and area covered. Most of the data is collected specifically for the target fields, although existing public datasets like PlantVillage may occasionally be reused to pre-train models. In general, the collected data remains private and belongs to the farmer, research institution, or agricultural company operating the drones. Access is restricted to authorized users such as data scientists or agronomists. Because agricultural data can reveal sensitive information, such as crop health or yield potential, strict data protection measures are applied, including encryption during transfer, anonymization of identifiable information, and restricted access control. Finally, the data is stored either on local servers for immediate analysis or in cloud-based storage systems for scalability and long-term archiving, with structured databases like PostgreSQL often used to manage spatial information efficiently.
The dataset used in YOLO-Fi comes from UAV-acquired multispectral imagery and derived vegetation indices. The study specifies that “Images of six bands and six vegetation indices [...] (Band1~Band6, NDVI, RVI, EVI, GNDVI, NDRE, and SAVI)... were extracted from UAV multispectral data”. To create a robust training dataset, “500 random samples were chosen from the UAV images, and the corresponding values of each feature for each sample point were acquired”. The data were then augmented, “both the fused images and RGB-displayed images undeàrwent cropping, rotation, scaling, mirroring, and enhancement to expand the dataset size. LabelMe was used to annotate sample datasets. Each dataset was split into training and validation sets in an 8:2 ratio”. In short, the study uses large, multispectral UAV datasets processed into labeled and enhanced image datasets for model training and validation, all managed through structured preprocessing, feature fusion, and annotation pipelines. [^2]

[^1]: disease detection in UAV Multispectral Images Optimized Image Registration and deep learning segmentation

[^2]: Precise extraction of targeted apple tree canopy with YOLO-Fi model for
advanced UAV spraying plans
